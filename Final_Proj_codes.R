##PROJECT
setwd("/Users/primordial/Desktop/Fall 2021/534")

library(gbm)

data_project <- read.csv("Data Project.csv", header = TRUE, sep = ",", dec = ".")
View(data_project)
table(data_project$default) 
str(data_project)


## INSTALLATION OF PACKAGES
install.packages("ggplot2")
library(ggplot2)
install.packages("data.table")
library(data.table)


data_project <- data_project[,-2] 
data_project <- data_project[,-2] 
View(data_project)
data_project$branch <- as.factor(data_project$branch) 

ggplot(data_project, aes(x = income, fill = default)) +
  geom_bar(aes(y = (..count..)/sum(..count..)))

ggplot(data_project, aes(x = employ, fill = default)) +
  geom_bar(aes(y = (..count..)/sum(..count..)))

ggplot(data_project, aes(x = ed, fill = default)) +
  geom_bar(aes(y = (..count..)/sum(..count..)))

qplot(ed, data=data_project, fill=default, bins=5)

boxplot(data_project$age, data=data_project, main="Distribution of age", ylab="Value of age")

boxplot(age~default, data=data_project, col= c("red","blue"), main="age vs default", ylab="age", xlab="default")

boxplot(adresse~default, data=data_project, col= c("magenta","cyan"), main="address vs default", ylab="address", xlab="default")

## INSTALLATION OF PACKAGES FOR RPART, C5.0, TREE CLASSIFIERS
install.packages("rpart")
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)
install.packages("C50")
library(C50)
install.packages("tree")
library(tree)

# separate data_project into 2/3 and 1/3 and generate the classifiers to test them

data_project_EA <- data_project[1:800,]
data_project_ET <- data_project[801:1200,]

#PLACING CLASSIFIERS ON THE LEARNING GROUP
treerpart <- rpart(default ~ ., data_project_EA) 
treeC50 <- C5.0(default ~ ., data_project_EA)
tree <- tree(default ~ ., data_project_EA)

# WE DRAW THE TREES GENERATED BY THE CLASSIFIERS
plot(treerpart)
text(treerpart, pretty = 0)
plot(treeC50, type="simple")
plot(tree)
text(tree, pretty = 0)

# WE TEST THE CLASSIFIERS ON THE TEST SET
test_treerpart <- predict(treerpart, data_project_ET, type="class")
print(test_treerpart)
table(test_treerpart)

test_treeC50 <- predict(treeC50, data_project_ET, type="class")
table(test_treeC50)

test_tree <- predict(tree, data_project_ET, type="class")
table(test_tree)

# WE LOOK AT THE SUCCESS RATES OF EACH CLASSIFIER ACCORDING TO THE TEST SET 
succes_treerpart <- as.data.frame(table(data_project_ET$default, test_treerpart))
colnames(succes_treerpart) = list("class", "Prediction", "Effective")
rate_succeesrpart <- sum(succes_treerpart[succes_treerpart$class==succes_treerpart$Prediction,"Effective"])/nrow(data_project_ET)
succes_treerpart
rate_succeesrpart

succes_treeC50 <- as.data.frame(table(data_project_ET$default, test_treeC50))
colnames(succes_treeC50) = list("class", "Prediction", "Effective")
rate_succeesC50 <- sum(succes_treeC50[succes_treeC50$class==succes_treeC50$Prediction,"Effective"])/nrow(data_project_ET)
succes_treeC50
rate_succeesC50

succes_tree <- as.data.frame(table(data_project_ET$default, test_tree))
colnames(succes_tree) = list("class", "Prediction", "Effective")
rate_succeestree <- sum(succes_tree[succes_tree$class==succes_tree$Prediction,"Effective"])/nrow(data_project_ET)
succes_tree
rate_succeestree

#INSTALLATION OF PACKAGES

install.packages("rpart.plot")
library(rpart.plot)
install.packages("rpart")
library(rpart)
install.packages("ROCR")
library(ROCR)




#MATRICE OF CONFUSION

mc_treerpart <- table(data_project_ET$default, test_treerpart)
mc_treeC50 <- table(data_project_ET$default, test_treeC50)
mc_tree <- table(data_project_ET$default, test_tree)

print(mc_treerpart) 
print(mc_treeC50) 
print(mc_tree)

#      NON    OUI
# NON  VN     FP
# OUI  FN     VP

# we want to minimize the number of incorrect positive predictions, so we look for the classifier with the lowest number of False Positives (top right): mc_tree1
# we want to minimize the number of incorrect negative predictions, so we look for the classifier with the lowest number of False Negatives (bottom left): mc_tree2
# WE LOOK AT SENSITIVITY FOR EACH CLASSIFIER
sensi_rpart <- mc_treerpart[2,2]/(mc_treerpart[2,2]+mc_treerpart[2,1])
#VP              #VP           #FN
sensi_C50 <- mc_treeC50[2,2]/(mc_treeC50[2,2]+mc_treeC50[2,1])
sensi_tree <- mc_tree[2,2]/(mc_tree[2,2]+mc_tree[2,1])

sensi_rpart
sensi_C50
sensi_tree


#NOW WATCHING THE PRECISION
preci_rpart <- mc_treerpart[2,2]/(mc_treerpart[2,2]+mc_treerpart[1,2])
#VP            #VP             #FP
preci_C50 <- mc_treeC50[2,2]/(mc_treeC50[2,2]+mc_treeC50[1,2])
preci_tree <- mc_tree[2,2]/(mc_tree[2,2]+mc_tree[1,2])

preci_rpart
preci_C50
preci_tree

speci_rpart <- mc_treerpart[1,1]/(mc_treerpart[1,1]+mc_treerpart[1,2])
speci_C50 <- mc_treeC50[1,1]/(mc_treeC50[1,1]+mc_treeC50[1,2])
speci_tree <- mc_tree[1,1]/(mc_tree[1,1]+mc_tree[1,2])

speci_rpart
speci_C50
speci_tree

# TRUE NEGATIVE RATE FOR EACH CLASSIFIER
rate_ofVN_rpart <- mc_treerpart[1,1]/(mc_treerpart[1,1]+mc_treerpart[2,1])
rate_ofVN_C50 <- mc_treeC50[1,1]/(mc_treeC50[1,1]+mc_treeC50[2,1])
rate_ofVN_tree <- mc_tree[1,1]/(mc_tree[1,1]+mc_tree[2,1])

rate_ofVN_rpart
rate_ofVN_C50
rate_ofVN_tree

#CALCULATE THE PROBABILITIES OF YES AND NO FOR EACH CLASSIFIER WITH RESPECT TO THE DEFAULT VARIABLE
prob_treerpart <- predict(treerpart, data_project_ET, type = "prob")
prob_treeC50 <- predict(treeC50, data_project_ET, type = "prob")
prob_tree <- predict(tree, data_project_ET, type = "vector")

print(prob_treerpart)
print(prob_treeC50)
print(prob_tree)  

prob_treerpart[,2]
prob_treerpart[,1]

df_resultrpart <- data.frame(data_project_ET$default, test_treerpart, prob_treerpart[,2], prob_treerpart[,1])
colnames(df_resultrpart) = list("classs", "Prediction", "P(Oui)", "P(Non)")

df_resultC50 <- data.frame(data_project_ET$default, test_treeC50, prob_treeC50[,2], prob_treeC50[,1])
colnames(df_resultC50) = list("classs", "Prediction", "P(Oui)", "P(Non)")

df_resulttree <- data.frame(data_project_ET$default, test_tree, prob_tree[,2], prob_tree[,1])
colnames(df_resulttree) = list("classs", "Prediction", "P(Oui)", "P(Non)")

summary(df_resultrpart[df_resultrpart$Prediction=="Oui", 3])
summary(df_resultrpart[df_resultrpart$Prediction=="Non", 3])

summary(df_resultC50[df_resultC50$Prediction=="Oui", 3])
summary(df_resultC50[df_resultC50$Prediction=="Non", 3])

summary(df_resulttree[df_resulttree$Prediction=="Oui", 3])
summary(df_resulttree[df_resulttree$Prediction=="Non", 3])

install.packages("ROCR")
detach("package:gplots", unload = TRUE)
library(gplots)
library(ROCR)

# WE WILL DEFINE THE ROC CURVES FOR EACH CLASSIFIER
roc_predrpart <- prediction(prob_treerpart[,2], data_project_ET$default)
roc_predC50 <- prediction(prob_treeC50[,2], data_project_ET$default)
roc_predtree <- prediction(prob_tree[,2], data_project_ET$default)

roc_perfrpart <- performance(roc_predrpart,"tpr","fpr")
roc_perfC50 <- performance(roc_predC50,"tpr","fpr")
roc_perftree <- performance(roc_predtree,"tpr","fpr")

plot(roc_perfrpart, col = "lightsalmon")
plot(roc_perfC50, col = "darkolivegreen3", add = TRUE)
plot(roc_perftree, col = "coral3", add = TRUE)

#HERE WE CHECK THE AUC INDICES VALUES FOR EACH CLASSIFIER
auc_treerpart <- performance(roc_predrpart, "auc")
auc_treeC50 <- performance(roc_predC50, "auc")
auc_tree <- performance(roc_predtree, "auc")

str(auc_treerpart)
str(auc_treeC50)
str(auc_tree)

attr(auc_treerpart, "y.values") #AUC rpart = 0.7773258
attr(auc_treeC50, "y.values") #AUC C50 = 0.7829583
attr(auc_tree, "y.values") #AUC tree = 0.7932301

install.packages("randomForest")
install.packages("kknn")
library(randomForest)
library(kknn)

#----------------#
# DEFINITION OF THE FUNCTION FOR THE RANDOM FORESTS CLASSIFIER #
#----------------#
# Learning, testing and evaluation function
test_rf <- function(arg1, arg2, arg3, arg4){
  # Learning the classifier
  rf <- randomForest(default~., data_project_EA, ntree = arg1, mtry = arg2)
  # Classifier test: predicted class
  rf_class <- predict(rf,data_project_ET, type="response") 
  print(table(data_project_ET$default, rf_class))
  # Classifier test: probabilities for each prediction
  rf_prob <- predict(rf, data_project_ET, type="prob")
  rf_pred <- prediction(rf_prob[,2], data_project_ET$default)
  rf_perf <- performance(rf_pred,"tpr","fpr")
  plot(rf_perf, main = "Random Forests randomForest()", add = arg3, col = arg4)
  rf_auc <- performance(rf_pred, "auc")
  cat("AUC = ", as.character(attr(rf_auc, "y.values")))
  invisible()
}

# Random therefore AUC values vary

# --------------------- #
# DEFINITION OF THE FUNCTION FOR THE K-NEAREST NEIGHBORS CLASSIFIER #
# --------------------- #
# Definition of the learning, test and evaluation function
test_knn <- function(arg1, arg2, arg3, arg4){

  knn <- kknn(default~., data_project_EA, data_project_ET, k = arg1, distance = arg2)
  print(table(data_project_ET$default, knn$fitted.values))
  knn_prob <- as.data.frame(knn$prob)
  knn_pred <- prediction(knn_prob$Oui, data_project_ET$default)
  knn_perf <- performance(knn_pred,"tpr","fpr")
  plot(knn_perf, main = "Classifiers KNN kknn()", add = arg3, col = arg4)
  knn_auc <- performance(knn_pred, "auc")
  cat("AUC = ", as.character(attr(knn_auc, "y.values")))
  invisible()
}

test_rf(300, 3, FALSE, "red") 
#We cannot give the best RF classifier because Random Forest contains a hazard and gives us a different AUC value for each draw
test_rf(300, 5, TRUE, "blue") 
test_rf(500, 3, TRUE, "green") 
test_rf(500, 5, TRUE, "orange")

test_knn(10, 1, FALSE, "red") 
test_knn(10, 2, TRUE, "blue") 
test_knn(20, 1, TRUE, "green") #AUC = 0.797202797202797
test_knn(20, 2, TRUE, "orange")

#INSTALLATION OF PACKAGES FOR SVM AND NAIVE BAYES CLASSIFIERS
install.packages("e1071")
install.packages("naivebayes")
install.packages("ROCR")

library(e1071) 
library(naivebayes) 
library(ROCR)

# DEFINITION OF THE FUNCTION FOR THE CLASSIFIER SUPPORT VECTOR MACHINES #
# ------------------------- #
# Definition of the learning, test and evaluation function
test_svm <- function(arg1, arg2, arg3){

  svm <- svm(default~., data_project_EA, probability=TRUE, kernel = arg1)
  svm_class <- predict(svm, data_project_ET, type="response") 
  print(table(data_project_ET$default, svm_class))
  svm_prob <- predict(svm, data_project_ET, probability=TRUE) 
  svm_prob <- attr(svm_prob, "probabilities")
  svm_pred <- prediction(svm_prob[,2], data_project_ET$default)
  svm_perf <- performance(svm_pred,"tpr","fpr")
  plot(svm_perf, main = "Support vector machines svm()", add = arg2, col = arg3)
  svm_auc <- performance(svm_pred, "auc")
  cat("AUC = ", as.character(attr(svm_auc, "y.values")))
  invisible()
}

# TEST Classifiers of Support vector machines
test_svm("linear", FALSE, "red") #AUC = 0.819433484803135
test_svm("polynomial", TRUE, "blue") 
test_svm("radial", TRUE, "green") 
test_svm("sigmoid", TRUE, "orange")

# ------------- #
# DEFINITION OF THE FUNCTION FOR THE NAIVE BAYES CLASSIFIER #
# ------------- #
# Definition of the learning, test and evaluation function
test_nb <- function(arg1, arg2, arg3, arg4){
  
  nb <- naive_bayes(default~., data_project_EA, laplace = arg1, usekernel = arg2)
  nb_class <- predict(nb, data_project_ET, type="class") 
  print(table(data_project_ET$default, nb_class))
  nb_prob <- predict(nb, data_project_ET, type="prob")
  nb_pred <- prediction(nb_prob[,2], data_project_ET$default)
  nb_perf <- performance(nb_pred,"tpr","fpr")
  plot(nb_perf, main = "Classifiers naiveBayes()", add = arg3, col
       = arg4)
  nb_auc <- performance(nb_pred, "auc")
  cat("AUC = ", as.character(attr(nb_auc, "y.values")))
  invisible()
}


#CLASSIFIER TEST Naive Bayes
test_nb(0, FALSE, FALSE, "red") 
test_nb(20, FALSE, TRUE, "blue") 
test_nb(0, TRUE, TRUE, "green") 
test_nb(20, TRUE, TRUE, "orange") #AUC = 0.788386710565698



#We will list all the AUCs of the classifiers to determine the best classifier

attr(auc_treerpart, "y.values") #AUC rpart = 0.7773258
attr(auc_treeC50, "y.values") #AUC C50 = 0.7829583
attr(auc_tree, "y.values") #AUC tree = 0.7932301
test_knn(20, 1, TRUE, "green") #AUC knn = 0.797202797202797
test_svm("linear", FALSE, "red") #AUC svm = 0.819433484803135
test_nb(20, TRUE, TRUE, "orange") #AUC nb = 0.788386710565698

#So SVM is the best classifier according to AUC criteria


install.packages("pROC")
library(pROC)


#Boosting Model
boost<-gbm(default~.,data_project_EA,distribution="gaussian",n.trees=5000,interaction.depth=4)
pred.boost<-predict(boost,data_project_ET,n.trees=5000,type="response")
pred.boost
boost_pred <- prediction(pred.boost, data_project_ET$default)
rocCurve.gbm <- roc(data_project_ET$default,pred.boost)
plot(rocCurve.gbm, col=c(3))
boost_auc <- performance(boost_pred, "auc")
cat("AUC = ", as.character(attr(boost_auc, "y.values")))


#INSTANCES TO PREDICT NEW DATA PROJECT
# data_project_new <- read.csv("Data-project-New.csv", header = TRUE, sep = ",", dec = ".")
# View(data_project_new)
# 
# data_project_new <- data_project_new[,-2] 
# data_project_new <- data_project_new[,-2] 
# data_project_new$branch <- as.factor(data_project_new$branch)


## APPLICATION OF SVM CLASSIFIERS ON NEW DATA PROJECT

# svm <- svm(default~., data_project, probability=TRUE, kernel = "linear")
# svm_class <- predict(svm, data_project_new, type="response")
# svm_prob <- predict(svm, data_project_new, probability=TRUE)
# svm_prob <- attr(svm_prob, "probabilities")
# svm_prob
# 
# data_project_new$Predictionsdefault <- svm_class
# View(data_project_new)
# 
# resultat_project <- read.csv("Data-project-New.csv", header = TRUE, sep = ",", dec = ".")
# resultat_project$Predictionsdefault <- svm_class
# resultat_project$ProbOUI <- svm_prob[,2]
# resultat_project$ProbNON <- svm_prob[,1]
# View(resultat_project)
# setnames(resultat_project, old=c("customer", "Predictionsdefault","ProbOUI","ProbNON"), new=c("Numero Client", "Predictions Defaut", "Proba OUI", "Proba NON"))
# View(resultat_project)
# resultat_project <- resultat_project[,-1]
# resultat_project <- resultat_project[,-1]
# for(i in 1:8)
# {
#   resultat_project <- resultat_project[,-2]
# }
# 
# df_resultproject <- data.frame(resultat_project$'Predictions Defaut', svm_class, svm_prob[,2], svm_prob[,1])
# colnames(df_resultproject) = list("classs", "Prediction", "P(Oui)", "P(Non)")
# 
# summary(df_resultproject)
# 
# # CREATION OF THE FINAL CSV FILE
# 
# write.table(resultat_project, file='ResultProject.csv', sep="\t", dec=".", row.names = F)